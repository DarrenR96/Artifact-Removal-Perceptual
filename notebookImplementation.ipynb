{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for Artefact Suppression\n",
    "\n",
    "There are two main networks:\n",
    "* Suppression Network\n",
    "* VQA Network\n",
    "\n",
    "In the suppression network:\n",
    "* Spatial Suppression Network\n",
    "* Temporal Suppression Network\n",
    "\n",
    "Both suppression networks use U-net architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Main imports: tensorflow, numpy, subprocess, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable GPU to dynamically allocate more memory if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computeDevices = tf.config.list_physical_devices('GPU')\n",
    "# print(computeDevices)\n",
    "# for device in computeDevices:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_memory_growth(device, True)\n",
    "#     except:\n",
    "#         print(f\"{device} cannot be set\")\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YUV420 Functions\n",
    "Helper functions to read and write YUV420 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readYUV420(name: str, resolution: tuple, upsampleUV: bool = False):\n",
    "    height = resolution[0]\n",
    "    width = resolution[1]\n",
    "    bytesY = int(height * width)\n",
    "    bytesUV = int(bytesY/4)\n",
    "    Y = []\n",
    "    U = []\n",
    "    V = []\n",
    "    with open(name,\"rb\") as yuvFile:\n",
    "        while (chunkBytes := yuvFile.read(bytesY + 2*bytesUV)):\n",
    "            Y.append(np.reshape(np.frombuffer(chunkBytes, dtype=np.uint8, count=bytesY, offset = 0), (width, height)))\n",
    "            U.append(np.reshape(np.frombuffer(chunkBytes, dtype=np.uint8, count=bytesUV, offset = bytesY),  (width//2, height//2)))\n",
    "            V.append(np.reshape(np.frombuffer(chunkBytes, dtype=np.uint8, count=bytesUV, offset = bytesY + bytesUV), (width//2, height//2)))\n",
    "    Y = np.stack(Y)\n",
    "    U = np.stack(U)\n",
    "    V = np.stack(V)\n",
    "    if upsampleUV:\n",
    "        U = U.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        V = V.repeat(2, axis=1).repeat(2, axis=2)\n",
    "    return Y, U, V\n",
    "\n",
    "\n",
    "def readYUV420Range(name: str, resolution: tuple, range: tuple, upsampleUV: bool = False):\n",
    "    height = resolution[0]\n",
    "    width = resolution[1]\n",
    "    bytesY = int(height * width)\n",
    "    bytesUV = int(bytesY/4)\n",
    "    Y = []\n",
    "    U = []\n",
    "    V = []\n",
    "    with open(name,\"rb\") as yuvFile:\n",
    "        startLocation = range[0]\n",
    "        endLocation = range[1] + 1\n",
    "        startLocationBytes = startLocation * (bytesY + 2*bytesUV)\n",
    "        endLocationBytes = endLocation * (bytesY + 2*bytesUV)\n",
    "        data = np.fromfile(yuvFile, np.uint8, endLocationBytes-startLocationBytes, offset=startLocationBytes).reshape(-1,bytesY + 2*bytesUV)\n",
    "        Y = np.reshape(data[:, :bytesY], (-1, width, height))\n",
    "        U = np.reshape(data[:, bytesY:bytesY+bytesUV], (-1, width//2, height//2))\n",
    "        V = np.reshape(data[:, bytesY+bytesUV:bytesY+2*bytesUV], (-1, width//2, height//2))\n",
    "    if upsampleUV:\n",
    "        U = U.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        V = V.repeat(2, axis=1).repeat(2, axis=2)\n",
    "    return Y, U, V\n",
    "\n",
    "\n",
    "def writeYUV420(name: str, Y, U, V, downsample=True):\n",
    "    towrite = bytearray()\n",
    "    if downsample:\n",
    "        U = U[:, ::2, ::2]\n",
    "        V = V[:, ::2, ::2]\n",
    "    for i in range(Y.shape[0]):\n",
    "        towrite.extend(Y[i].tobytes())\n",
    "        towrite.extend(U[i].tobytes())\n",
    "        towrite.extend(V[i].tobytes())\n",
    "    with open(name, \"wb\") as destination:\n",
    "        destination.write(towrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "Data generator that feeds in data of a specific batch size to Suppression Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, referencePaths, degradedPaths, frameRanges, batch_size, dim, shuffle=True):\n",
    "        self.referencePaths = referencePaths\n",
    "        self.degradedPaths = degradedPaths\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.frameRanges = frameRanges\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.referencePaths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.referencePaths)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        referencePaths_temp = [] \n",
    "        degradedPaths_temp = []\n",
    "        frameRanges_temp = []\n",
    "        for i in indexes:\n",
    "            referencePaths_temp.append(self.referencePaths[i])\n",
    "            degradedPaths_temp.append(self.degradedPaths[i])\n",
    "            frameRanges_temp.append(self.frameRanges[i])\n",
    "        \n",
    "        X, y = self.__data_generation(referencePaths_temp, degradedPaths_temp, frameRanges_temp)\n",
    "        \n",
    "        return X,y \n",
    "    \n",
    "    def __data_generation(self, referencePaths_temp, degradedPaths_temp, frameRanges_temp):\n",
    "        X = np.empty([self.batch_size, 5,*self.dim])\n",
    "        y = np.empty([self.batch_size, 3,*self.dim])\n",
    "        \n",
    "        for i, (degradedPath, frameRange) in enumerate(zip(degradedPaths_temp, frameRanges_temp)):\n",
    "            Ydeg, Udeg, Vdeg = readYUV420Range(degradedPath, (1920,1080), frameRange, upsampleUV = True)\n",
    "            YUVdeg = np.stack([Ydeg, Udeg, Vdeg], axis=-1)\n",
    "            X[i] = YUVdeg\n",
    "        \n",
    "        for i, (referencePath, frameRange) in enumerate(zip(referencePaths_temp, frameRanges_temp)):\n",
    "            Yref, Uref, Vref = readYUV420Range(referencePath, (1920,1080), (frameRange[0]+1, frameRange[1]-1), upsampleUV = True)\n",
    "            YUVref = np.stack([Yref,Uref,Vref], axis=-1)\n",
    "            y[i] = YUVref\n",
    "        \n",
    "        X = (X/255).astype(np.float64)\n",
    "        y = (y/255).astype(np.float64)\n",
    "        \n",
    "        return X, y\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "Loss functions for MSE and perceptual losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, x):\n",
    "    loss = tf.reduce_mean(tf.square(tf.subtract(y,x)))\n",
    "    return loss\n",
    "\n",
    "def perceptCompute(ref, pred, path='/home/ramsookd/ArtefactReduction/processingTemp/'):\n",
    "    ref = ref * 255\n",
    "    pred = tf.round(tf.clip_by_value(pred, 0, 1) * 255)\n",
    "    \n",
    "    ref = ref.astype(np.uint8)\n",
    "    pred = pred.numpy().astype(np.uint8)\n",
    "    \n",
    "    refFile = f'{path}ref_temp.yuv'\n",
    "    predFile = f'{path}pred_temp.yuv'\n",
    "    \n",
    "    writeYUV420(f'{refFile}', ref[:,:,:,0], ref[:,:,:,1], ref[:,:,:,2], downsample=True)\n",
    "    writeYUV420(f'{predFile}', pred[:,:,:,0], pred[:,:,:,1], pred[:,:,:,2], downsample=True)\n",
    "    \n",
    "    commandVMAF = f\"vmaf --width 1920 --height 1080 -p 420 -b 8 -m version=vmaf_v0.6.1 -o {path}CSV.csv --csv -r {refFile} -d {predFile}\"\n",
    "    runVMAF = subprocess.Popen(commandVMAF,stderr=subprocess.PIPE, stdout=subprocess.PIPE, shell=True, cwd='/')\n",
    "    processOut, processErr = runVMAF.communicate() \n",
    "\n",
    "    vmafDF = pd.read_csv(f\"{path}CSV.csv\")\n",
    "    vmafScore = vmafDF['vmaf'].mean()\n",
    "    return vmafScore\n",
    "\n",
    "def l2Loss(x, tensor):\n",
    "    return tf.reduce_mean((tf.square(tensor-x)))\n",
    "\n",
    "def perceptualLoss(alpha, ref_frames, pred_frames, vqa_pred):\n",
    "\n",
    "    batch_size = ref_frames.shape[0]\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        Y_frames_ref = ref_frames[i, :, :, :, 0]\n",
    "        UV_frames_ref = ref_frames[i, :, :, :, 1:]\n",
    "        \n",
    "        Y_frames_pred = pred_frames[i, :, :, :, 0]\n",
    "        UV_frames_pred = pred_frames[i, :, :, :, 1:]\n",
    "        \n",
    "        print(vqa_pred[i])\n",
    "        loss = (alpha*(l2Loss(100,vqa_pred[i])) + (1-alpha)*mse(Y_frames_ref, Y_frames_pred)) + (mse(UV_frames_ref,UV_frames_pred))\n",
    "        loss += loss\n",
    "\n",
    "    loss = loss/batch_size\n",
    "    return loss\n",
    "\n",
    "def vqaLoss(ref_frames, pred_frames, pred_vmaf):\n",
    "    batch_size = ref_frames.shape[0]\n",
    "    actual_vmaf = []\n",
    "    for i in range(batch_size):\n",
    "        actual_vmaf.append(perceptCompute(ref_frames[i], pred_frames[i]))\n",
    "    actual_vmaf = tf.convert_to_tensor(actual_vmaf)\n",
    "    print(actual_vmaf)\n",
    "    print(pred_vmaf)\n",
    "    return(mse(pred_vmaf, actual_vmaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBNReluDown(layers.Layer):\n",
    "    def __init__(self, numFilters, size, strides, bn=False, **kwargs):\n",
    "        super(CNNBNReluDown, self).__init__()\n",
    "        self.numFilters = numFilters\n",
    "        self.size = size\n",
    "        self.bn = bn\n",
    "        self.strides = strides\n",
    "        self.convLayer = layers.Conv2D(self.numFilters, self.size, strides=self.strides ,padding=\"same\")\n",
    "        if self.bn:\n",
    "            self.bnLayer = layers.BatchNormalization()\n",
    "        self.reluLayer = layers.LeakyReLU()\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'numFilters' : self.numFilters,\n",
    "            'size' : self.size,\n",
    "            'strides' : self.strides,\n",
    "            'bn' : self.bn\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.convLayer(inputs)\n",
    "        if self.bn:\n",
    "            x = self.bnLayer(x, training=training)\n",
    "        x = self.reluLayer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNBNReluUp(layers.Layer):\n",
    "    def __init__(self, numFilters, size, strides, dropout=False, **kwargs):\n",
    "        super(CNNBNReluUp, self).__init__()\n",
    "        self.numFilters = numFilters\n",
    "        self.size = size\n",
    "        self.dropout = dropout\n",
    "        self.strides = strides\n",
    "        self.convLayer = layers.Conv2DTranspose(self.numFilters, self.size, strides=self.strides, padding=\"same\")\n",
    "        if self.dropout:\n",
    "            self.dropoutLayer = layers.Dropout(0.3)\n",
    "        self.reluLayer = layers.LeakyReLU()\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'numFilters' : self.numFilters,\n",
    "            'size' : self.size,\n",
    "            'strides' : self.strides,\n",
    "            'dropout' : self.dropout\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.convLayer(inputs)\n",
    "        if self.dropout:\n",
    "            x = self.dropoutLayer(x, training=training)\n",
    "        x = self.reluLayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialSuppression(keras.Model):\n",
    "    def __init__(self, encoder, decoder, outChannels = 3):\n",
    "        super(SpatialSuppression, self).__init__()\n",
    "        self.numEncoderBlocks = len(encoder)\n",
    "        self.numDecoderBlocks = len(decoder)\n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        for encoder_opts in encoder:\n",
    "            self.encoder.append(CNNBNReluDown(encoder_opts[0],encoder_opts[1],encoder_opts[2]))\n",
    "        \n",
    "        for decoder_opts in decoder:\n",
    "            self.decoder.append(CNNBNReluUp(decoder_opts[0],decoder_opts[1],decoder_opts[2]))\n",
    "        self.lastConv = layers.Conv2DTranspose(outChannels, 4, strides=2, padding=\"same\")\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        skips = []\n",
    "        for encoderLayer in self.encoder:\n",
    "            x = encoderLayer(x, training=training)\n",
    "            skips.append(x)\n",
    "        \n",
    "        skips = reversed(skips[:-1])\n",
    "        \n",
    "        for decoderLayer, skip in zip(self.decoder, skips):\n",
    "            x = decoderLayer(x, training=training)\n",
    "            x = layers.Concatenate()([x, skip])\n",
    "    \n",
    "        x = self.lastConv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = keras.Input(shape=(1080,1920,3))\n",
    "        return keras.Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "\n",
    "class CNN3D_BNReluDown(layers.Layer):\n",
    "    def __init__(self, numFilters, size, strides, bn=False,  **kwargs):\n",
    "        super(CNN3D_BNReluDown, self).__init__()\n",
    "        self.numFilters = numFilters\n",
    "        self.size = size\n",
    "        self.bn = bn\n",
    "        self.strides = strides\n",
    "        self.conv3DLayer = layers.Conv3D(self.numFilters, self.size, strides=self.strides ,padding=\"same\")\n",
    "        if self.bn:\n",
    "            self.bnLayer = layers.BatchNormalization()\n",
    "        self.reluLayer = layers.LeakyReLU()\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'numFilters' : self.numFilters,\n",
    "            'size' : self.size,\n",
    "            'strides' : self.strides,\n",
    "            'bn' : self.bn\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv3DLayer(inputs)\n",
    "        if self.bn:\n",
    "            x = self.bnLayer(x, training=training)\n",
    "        x = self.reluLayer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN3D_BNReluUp(layers.Layer):\n",
    "    def __init__(self, numFilters, size, strides, dropout=False,  **kwargs):\n",
    "        super(CNN3D_BNReluUp, self).__init__()\n",
    "        self.numFilters = numFilters\n",
    "        self.size = size\n",
    "        self.dropout = dropout\n",
    "        self.strides = strides\n",
    "        self.conv3DLayer = layers.Conv3DTranspose(self.numFilters, self.size, strides=self.strides, padding=\"same\")\n",
    "        if self.dropout:\n",
    "            self.dropoutLayer = layers.Dropout(0.3)\n",
    "        self.reluLayer = layers.LeakyReLU()\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'numFilters' : self.numFilters,\n",
    "            'size' : self.size,\n",
    "            'strides' : self.strides,\n",
    "            'dropout' : self.dropout\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv3DLayer(inputs)\n",
    "        if self.dropout:\n",
    "            x = self.dropoutLayer(x, training=training)\n",
    "        x = self.reluLayer(x)\n",
    "        return x\n",
    "\n",
    "class TemporalSuppression(keras.Model):\n",
    "    def __init__(self, encoder, decoder, outChannels = 3):\n",
    "        super(TemporalSuppression, self).__init__()\n",
    "        self.numEncoderBlocks = len(encoder)\n",
    "        self.numDecoderBlocks = len(decoder)\n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        with tf.device('/device:GPU:1'):\n",
    "            for encoder_opts in encoder:\n",
    "                self.encoder.append(CNN3D_BNReluDown(encoder_opts[0],encoder_opts[1],encoder_opts[2],encoder_opts[3]))\n",
    "            \n",
    "        with tf.device('/device:GPU:2'):\n",
    "            for decoder_opts in decoder:\n",
    "                self.decoder.append(CNN3D_BNReluUp(decoder_opts[0],decoder_opts[1],decoder_opts[2],decoder_opts[3]))\n",
    "            self.lastConv = layers.Conv3DTranspose(outChannels, 4, strides=(1,2,2), padding=\"same\")\n",
    "            self.lastReLU = layers.LeakyReLU()\n",
    "            self.lastConv3D = layers.Conv3D(outChannels,3,strides=(3,1,1), padding=\"same\")\n",
    "            self.lastReLUFinal = layers.LeakyReLU()\n",
    "\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        \n",
    "        # skips = []\n",
    "\n",
    "        with tf.device('/device:GPU:1'):\n",
    "            for encoderLayer in self.encoder:\n",
    "                x = encoderLayer(x, training=training)\n",
    "                # skips.append(x)\n",
    "            \n",
    "            # skips = reversed(skips[:-1])\n",
    "    \n",
    "        with tf.device('/device:GPU:2'):\n",
    "\n",
    "            # for decoderLayer, skip in zip(self.decoder, skips):\n",
    "            for decoderLayer in self.decoder:\n",
    "                x = decoderLayer(x, training=training)\n",
    "                # x = layers.Concatenate()([x, skip])\n",
    "        \n",
    "            x = self.lastConv(x, training=training)\n",
    "            x = self.lastReLU(x, training=training)\n",
    "            x = self.lastConv3D(x, training=training)\n",
    "            x = self.lastReLUFinal(x, training=training)\n",
    "            x = tf.clip_by_value(x, 0, 1)\n",
    "            return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = keras.Input(shape=(3,1080,1920,3))\n",
    "        return keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "class VideoQualityAssessment(keras.Model):\n",
    "    def __init__(self, spatialBlocks, temporalBlock, finalBlock, denseBlock):\n",
    "        super(VideoQualityAssessment, self).__init__()\n",
    "        self.spatialBlocks = []\n",
    "        self.temporalBlock = []\n",
    "        self.finalBlock = []\n",
    "        self.denseBlock = []\n",
    "        \n",
    "        for spatial in spatialBlocks:\n",
    "            self.spatialBlocks.append(CNNBNReluDown(spatial[0],spatial[1],spatial[2], spatial[3]))\n",
    "        \n",
    "        for temporal in temporalBlock:\n",
    "            self.temporalBlock.append(CNNBNReluDown(temporal[0],temporal[1],temporal[2], temporal[3]))\n",
    "        \n",
    "        for final in finalBlock:\n",
    "            self.finalBlock.append(CNNBNReluDown(final[0],final[1],final[2], final[3]))\n",
    "        \n",
    "        for dense in denseBlock:\n",
    "            self.denseBlock.append(layers.Dense(dense))\n",
    "    \n",
    "    def call(self, x_ref_min1, x_ref, x_ref_pl1, x_dist_min1, x_dist, x_dist_pl1, training = False):\n",
    "        for spatial in self.spatialBlocks:\n",
    "            x_ref_min1 = spatial(x_ref_min1, training=training)\n",
    "        for spatial in self.spatialBlocks:\n",
    "            x_ref = spatial(x_ref, training=training)\n",
    "        for spatial in self.spatialBlocks:\n",
    "            x_ref_pl1 = spatial(x_ref_pl1, training=training)\n",
    "            \n",
    "        x_ref = layers.Concatenate()([x_ref_min1, x_ref, x_ref_pl1])\n",
    "        \n",
    "        for spatial in self.spatialBlocks:\n",
    "            x_dist_min1 = spatial(x_dist_min1, training=training)\n",
    "        for spatial in self.spatialBlocks:\n",
    "            x_dist = spatial(x_dist, training=training)\n",
    "        for spatial in self.spatialBlocks:\n",
    "            x_dist_pl1 = spatial(x_dist_pl1, training=training)\n",
    "        \n",
    "        x_dist = layers.Concatenate()([x_dist_min1, x_dist, x_dist_pl1])\n",
    "                \n",
    "        for temporal in self.temporalBlock:\n",
    "            x_ref = temporal(x_ref, training=training)\n",
    "        for temporal in self.temporalBlock:\n",
    "            x_dist = temporal(x_dist, training=training)\n",
    "            \n",
    "        x = layers.Concatenate()([x_ref, x_dist])\n",
    "        \n",
    "        for final in self.finalBlock:\n",
    "            x = final(x, training=training)\n",
    "        \n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        for dense in self.denseBlock:\n",
    "            x = dense(x, training=training)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x_ref_min1 = keras.Input(shape=(1080,1920,3))\n",
    "        x_ref = keras.Input(shape=(1080,1920,3))\n",
    "        x_ref_pl1 = keras.Input(shape=(1080,1920,3))\n",
    "\n",
    "        x_dist_min1 = keras.Input(shape=(1080,1920,3))\n",
    "        x_dist = keras.Input(shape=(1080,1920,3))\n",
    "        x_dist_pl1 = keras.Input(shape=(1080,1920,3))\n",
    "        \n",
    "        return keras.Model(inputs=[x_ref_min1, x_ref, x_ref_pl1, x_dist_min1, x_dist, x_dist_pl1], \n",
    "                           outputs=self.call(x_ref_min1, x_ref, x_ref_pl1, x_dist_min1, x_dist, x_dist_pl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialBlock = [\n",
    "    (64, 3, 2, True),\n",
    "    (64, 3, 2, False),\n",
    "    (128, 3, 2, False),\n",
    "    (128, 3, 2, False),\n",
    "]\n",
    "\n",
    "temporalBlock = [\n",
    "    (128, 3, 2, False),\n",
    "    (128, 3, 2, False),\n",
    "    (256, 3, 2, False),\n",
    "]\n",
    "\n",
    "finalBlock = [\n",
    "    (256, 3, 2, False),\n",
    "    (256, 3, 2, False),\n",
    "    (512, 3, 2, False),\n",
    "]\n",
    "\n",
    "denseBlock = [\n",
    "    1024,\n",
    "    512,\n",
    "    128,\n",
    "    1\n",
    "]\n",
    "\n",
    "with tf.device('/device:GPU:3'):\n",
    "    vqaModel = VideoQualityAssessment(spatialBlock,temporalBlock, finalBlock, denseBlock).model()\n",
    "\n",
    "\n",
    "\n",
    "encoderTemporal = [\n",
    "    (64, 3, (1,2,2), True),\n",
    "    (64, 3, (1,2,2), False),\n",
    "    (128, 3, (1,2,2), False),\n",
    "    (128, 5, (1,5,5), False),\n",
    "    (256, 3, (1,3,3), False),\n",
    "    (256, 3, (1,3,2), False),\n",
    "    (256, 3, (1,3,2), False),\n",
    "    (512, 3, (1,2,2), False),\n",
    "]\n",
    "\n",
    "decoderTemporal = [\n",
    "    (256, 3, (1,1,2), True),\n",
    "    (256, 3, (1,3,2), True),\n",
    "    (256, 3, (1,3,2), True),\n",
    "    (128, 3, (1,3,3), False),\n",
    "    (128, 5, (1,5,5), False),\n",
    "    (64, 3, (1,2,2), False),\n",
    "    (64, 3, (1,2,2), False),\n",
    "]\n",
    "\n",
    "with tf.device('/device:GPU:2'):\n",
    "    temporalModel = TemporalSuppression(encoderTemporal, decoderTemporal).model()\n",
    "\n",
    "encoderSpatial = [\n",
    "    (64, 4, 2, True),\n",
    "    (64, 4, 2, False),\n",
    "    (128, 4, 2, False),\n",
    "    (128, 5, 5, False),\n",
    "    (256, 4, 3, False),\n",
    "    (256, 4, (3,2), False),\n",
    "    (512, 3, (3,2), False),\n",
    "    (512, 3, 2, False),\n",
    "]\n",
    "\n",
    "decoderSpatial = [\n",
    "    (512, 3, (1,2), True),\n",
    "    (256, 4, (3,2), True),\n",
    "    (256, 4, (3,2), True),\n",
    "    (128, 4, (3,3), False),\n",
    "    (128, 5, 5, False),\n",
    "    (64, 4, 2, False),\n",
    "    (64, 4, 2, False),\n",
    "]\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    spatialModel = SpatialSuppression(encoderSpatial, decoderSpatial).model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data prep\n",
    "videoDF = pd.read_csv(\"/home/ramsookd/ArtefactReduction/data/windowedDataset.csv\")\n",
    "refFiles = videoDF['cleanPath'].tolist()[1001:1002]\n",
    "degFiles = videoDF['degradedPath'].tolist()[1001:1002]\n",
    "frameStart = videoDF['StartFrame'].tolist()[1001:1002]\n",
    "frameEnd = videoDF['EndFrame'].tolist()[1001:1002]\n",
    "frames = [(x, y) for x, y in zip(frameStart, frameEnd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Loop\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "vqaTrainLoss = tf.keras.metrics.Mean('vqaTrainLoss', dtype=tf.float32)\n",
    "suppressionTrainLoss = tf.keras.metrics.Mean('suppressionTrainLoss', dtype=tf.float32)\n",
    "\n",
    "\n",
    "EPOCHS = 5 \n",
    "batch_size = 1\n",
    "dims = (1080, 1920, 3)\n",
    "alpha = 1e-5\n",
    "\n",
    "opt_vqa = keras.optimizers.Adam(1e-4)\n",
    "opt_suppresion = keras.optimizers.Adam(1e-2)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    dataGen = DataGenerator(refFiles, degFiles, frames, batch_size, dims, True)\n",
    "    batchSteps = dataGen.__len__()\n",
    "    for i in range(batchSteps):\n",
    "        X, y = dataGen.__getitem__(i)\n",
    "        \n",
    "        ## Train suppressionNet\n",
    "        with tf.GradientTape() as suppressionTape:\n",
    "            with tf.device('/device:GPU:1'):\n",
    "                XSpa = spatialModel(tf.reshape(X, [batch_size*5, *dims]), training=True)\n",
    "                del X\n",
    "                XSpa = tf.reshape(XSpa, [batch_size, 5, *dims])\n",
    "                tempIn = []\n",
    "                for bs in range(batch_size):\n",
    "                    for frameCenter in range(1,4):\n",
    "                        if frameCenter == 4:\n",
    "                            tempIn.append(XSpa[bs,frameCenter-1:,:,:,:])\n",
    "                        if frameCenter != 4:\n",
    "                            tempIn.append(XSpa[bs,frameCenter-1:frameCenter+2,:,:,:])\n",
    "                del XSpa\n",
    "                tempIn = tf.stack(tempIn, axis=0)\n",
    "                \n",
    "            with tf.device('/device:GPU:2'):\n",
    "                tempOut = temporalModel(tempIn, training=True)\n",
    "                tempOut = tf.reshape(tempOut, [batch_size, 3, *dims])\n",
    "            with tf.device('/device:GPU:3'):\n",
    "                vqaPred = vqaModel([y[:,0,:,:,:], y[:,1,:,:,:], y[:,2,:,:,:], tempOut[:,0,:,:,:], tempOut[:,1,:,:,:], tempOut[:,2,:,:,:]],training=False)\n",
    "            lossSuppression = perceptualLoss(alpha,y,tempOut,vqaPred)\n",
    "        grads = suppressionTape.gradient(lossSuppression, spatialModel.trainable_weights+temporalModel.trainable_weights)\n",
    "        opt_suppresion.apply_gradients(zip(grads, spatialModel.trainable_weights+temporalModel.trainable_weights))\n",
    "\n",
    "        ## Train vqaNet\n",
    "        with tf.GradientTape() as vqaTape:\n",
    "            with tf.device('/device:GPU:3'):\n",
    "                vqaPred = vqaModel([y[:,0,:,:,:], y[:,1,:,:,:], y[:,2,:,:,:], tempOut[:,0,:,:,:], tempOut[:,1,:,:,:], tempOut[:,2,:,:,:]],training=True)\n",
    "            lossVQA = vqaLoss(y, tempOut, vqaPred)\n",
    "        grads = vqaTape.gradient(lossVQA, vqaModel.trainable_weights)\n",
    "        opt_vqa.apply_gradients(zip(grads, vqaModel.trainable_weights))\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {epoch}, step: {i}, VQA Loss: {lossVQA.numpy()}, Suppression Loss: {lossSuppression.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
